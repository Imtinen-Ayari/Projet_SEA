{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9e14ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of https://www.wikipedia.org: Wikipedia\n",
      "Title of https://www.youtube.com: YouTube\n",
      "Title of https://www.reddit.com: Reddit - Dive into anything\n",
      "Total execution time: 2.50 seconds\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue\n",
    "import time\n",
    "import csv\n",
    "import urllib.robotparser\n",
    "\n",
    "NUM_THREADS = 5\n",
    "url_queue = Queue()\n",
    "urls_to_crawl = [\n",
    "    'https://www.youtube.com',\n",
    "    'https://www.wikipedia.org',\n",
    "    'https://www.reddit.com'\n",
    "]\n",
    "\n",
    "output_file = 'crawl_results.csv'\n",
    "\n",
    "# Initialisation du fichier CSV\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['URL', 'Title'])\n",
    "\n",
    "def can_fetch(url):\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(url + '/robots.txt')\n",
    "    rp.read()\n",
    "    return rp.can_fetch('*', url)\n",
    "\n",
    "def crawl(url):\n",
    "    if not can_fetch(url):\n",
    "        print(f'Crawling disallowed by robots.txt for {url}')\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.title.string if soup.title else 'No title'\n",
    "        with threading.Lock():\n",
    "            with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([url, title])\n",
    "        print(f'Title of {url}: {title}')\n",
    "    except requests.RequestException as e:\n",
    "        print(f'Error crawling {url}: {e}')\n",
    "\n",
    "def worker():\n",
    "    while True:\n",
    "        url = url_queue.get()\n",
    "        if url is None:\n",
    "            break\n",
    "        crawl(url)\n",
    "        time.sleep(1)  # Pause d'une seconde entre les requêtes\n",
    "        url_queue.task_done()\n",
    "\n",
    "# Fonction pour mesurer le temps d'exécution\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    threads = []\n",
    "    for _ in range(NUM_THREADS):\n",
    "        t = threading.Thread(target=worker)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for url in urls_to_crawl:\n",
    "        url_queue.put(url)\n",
    "\n",
    "    url_queue.join()\n",
    "\n",
    "    for _ in range(NUM_THREADS):\n",
    "        url_queue.put(None)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'Total execution time: {elapsed_time:.2f} seconds')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49b82a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of https://www.wikipedia.org: Wikipedia\n",
      "Title of https://www.reddit.com: Reddit - Dive into anything\n",
      "Title of https://en.wikipedia.org/wiki/List_of_Wikipedia_mobile_applications: List of Wikipedia mobile applications - Wikipedia\n",
      "Crawling disallowed by robots.txt for https://creativecommons.org/licenses/by-sa/4.0/\n",
      "Title of https://meta.wikimedia.org/wiki/Special:MyLanguage/List_of_Wikipedias: List of Wikipedias - Meta\n",
      "Title of https://donate.wikimedia.org/?utm_medium=portal&utm_campaign=portalFooter&utm_source=portalFooter: Make your donation now - Wikimedia Foundation\n",
      "Title of https://meta.wikimedia.org/wiki/Terms_of_use: Terms of use - Meta\n",
      "Title of https://meta.wikimedia.org/wiki/Privacy_policy: Privacy policy - Meta\n",
      "Title of https://www.reddit.com/login/: Reddit - Dive into anything\n",
      "Title of https://accounts.reddit.com/adsregister?utm_source=web3x_consumer&utm_name=user_menu_cta: \n",
      "            \n",
      "                Sign Up for Reddit Ads\n",
      "            \n",
      "        \n",
      "Title of https://play.google.com/store/apps/details?id=org.wikipedia&referrer=utm_source%3Dportal%26utm_medium%3Dbutton%26anid%3Dadmob: Wikipedia - Apps on Google Play\n",
      "Title of https://play.google.com/store/apps/details?id=com.reddit.frontpage: Reddit - Apps on Google Play\n",
      "Title of https://itunes.apple.com/app/apple-store/id324715238?pt=208305&ct=portal&mt=8: \n",
      "      âWikipedia on the AppÂ Store\n",
      "    \n",
      "Title of https://apps.apple.com/US/app/id1064216828: \n",
      "      âReddit on the AppÂ Store\n",
      "    \n",
      "Title of https://reddit.com/t/amazing/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/interesting/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/funny/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/animals_and_pets/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/cringe_and_facepalm/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/memes/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/reddit_meta/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/oddly_satisfying/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/wholesome_and_heartwarming/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/action_games/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/gaming_consoles_and_gear/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/esports/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/adventure_games/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/other_games/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/gaming_news_and_discussion/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/role_playing_games/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/simulation_games/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/mobile_games/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/strategy_games/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/sports_and_racing_games/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/q_and_as/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/stories_and_confessions/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/tabletop_games/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/artificial_intelligence_and_machine_learning/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/3d_printing/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/computers_and_hardware/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/diy_electronics/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/software_and_apps/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/programming/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/consumer_electronics/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/virtual_and_augmented_reality/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/tech_news_and_discussion/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/streaming_services/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/creators_and_influencers/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/celebrities/: Reddit - Dive into anything\n",
      "Title of https://reddit.com/t/generations_and_nostalgia/: Reddit - Dive into anything\n",
      "Total execution time: 66.04 seconds\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue\n",
    "import time\n",
    "import csv\n",
    "import urllib.robotparser\n",
    "\n",
    "NUM_THREADS = 5\n",
    "MAX_URLS = 50  # Limite le nombre d'URLs à crawler\n",
    "url_queue = Queue()\n",
    "visited_urls = set()\n",
    "initial_urls = [\n",
    "    'https://www.wikipedia.org',\n",
    "    'https://www.reddit.com'\n",
    "]\n",
    "\n",
    "output_file = 'crawl_results.csv'\n",
    "\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['URL', 'Title'])\n",
    "\n",
    "def can_fetch(url):\n",
    "    try:\n",
    "        rp = urllib.robotparser.RobotFileParser()\n",
    "        rp.set_url(url + '/robots.txt')\n",
    "        rp.read()\n",
    "        return rp.can_fetch('*', url)\n",
    "    except Exception as e:\n",
    "        print(f'Error reading robots.txt for {url}: {e}')\n",
    "        return False\n",
    "\n",
    "def crawl(url):\n",
    "    if not can_fetch(url):\n",
    "        print(f'Crawling disallowed by robots.txt for {url}')\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.title.string if soup.title else 'No title'\n",
    "        with threading.Lock():\n",
    "            with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([url, title])\n",
    "        print(f'Title of {url}: {title}')\n",
    "        \n",
    "        # Find and queue internal links\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('http'):\n",
    "                if href not in visited_urls and len(visited_urls) < MAX_URLS:\n",
    "                    visited_urls.add(href)\n",
    "                    url_queue.put(href)\n",
    "    except requests.RequestException as e:\n",
    "        print(f'Error crawling {url}: {e}')\n",
    "\n",
    "def worker():\n",
    "    while True:\n",
    "        url = url_queue.get()\n",
    "        if url is None:\n",
    "            break\n",
    "        crawl(url)\n",
    "        # Reduce delay between requests to 0.5 to 1 second\n",
    "        time.sleep(0.5)\n",
    "        url_queue.task_done()\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    threads = []\n",
    "    for _ in range(NUM_THREADS):\n",
    "        t = threading.Thread(target=worker)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for url in initial_urls:\n",
    "        url_queue.put(url)\n",
    "        visited_urls.add(url)\n",
    "\n",
    "    url_queue.join()\n",
    "\n",
    "    for _ in range(NUM_THREADS):\n",
    "        url_queue.put(None)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'Total execution time: {elapsed_time:.2f} seconds')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc16acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
